---
title: "Homework 3: Case 1 and COVID Case Study"
author:
- Wendy Deng
- Ruolan Li
- Kira Nightingale
date: 'Due before midnight, Feb 26'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    extra_dependencies: ["dcolumn"]
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

```{r Setup, echo=FALSE, results='hide', warning=FALSE}
knitr::opts_chunk$set(echo = T, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output

# Package setup
if(!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate,
               GGally, RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, ISLR, readxl, magrittr, glmnet)
```

\pagebreak

# Overview

Multiple regression is one of the most popular methods used in
statistics as well as in machine learning. We use linear models as a
working model for its simplicity and interpretability. It is important
that we use domain knowledge as much as we could to determine the form
of the response as well as the function format for the factors. Then,
when we have many possible features to be included in the working model
it is inevitable that we need to choose a best possible model with a
sensible criterion. `Cp`, `BIC` and regularizations such as LASSO are
introduced. Be aware that if a model selection is done formally or
informally, the inferences obtained with the final `lm()` fit may not be
valid. Some adjustment will be needed. This last step is beyond the
scope of this class. Check the current research line that Linda and
collaborators are working on.

This homework consists of two parts: the first one is an exercise (you
will feel it being a toy example after the covid case study) to get
familiar with model selection skills such as, `Cp` and `BIC`. The main
job is a rather involved case study about devastating covid19 pandemic.
Please read through the case study first. This project is for sure a
great one listed in your CV.

For covid case study, the major time and effort would be needed in EDA
portion.

## Objectives

-   Model building process

-   Methods

    -   Model selection
        -   All subsets
        -   Forward/Backward
    -   Regularization
        -   LASSO (L1 penalty)
        -   Ridge (L2 penalty)
        -   Elastic net

-   Understand the criteria

    -   `Cp`
    -   Testing Errors
    -   `BIC`
    -   `K fold Cross Validation`
    -   `LASSO`

-   Packages

    -   `lm()`, `Anova`
    -   `regsubsets()`
    -   `glmnet()` & `cv.glmnet()`

# Review materials

-   Study lecture: Model selection
-   Study lecture: Regularization
-   Study lecture: Multiple regression

Review the code and concepts covered during lectures: multiple
regression, model selection and penalized regression through elastic
net.

# Homework 2, Case study 3: Auto data set

If you haven't done this as part of the homework 2, please attach it
here.

# Case study 1: `ISLR::Auto` data

This will be the last part of the Auto data from ISLR. The original data
contains 408 observations about cars. It has some similarity as the Cars
data that we use in our lectures. To get the data, first install the
package `ISLR`. The data set `Auto` should be loaded automatically. We
use this case to go through methods learned so far.

Final modelling question: We want to explore the effects of each feature
as best as possible.

1)  Preparing variables:

```{=html}
<!-- -->
```
a)  You may explore the possibility of variable transformations. We
    normally do not suggest to transform $x$ for the purpose of
    interpretation. You may consider to transform $y$ to either correct
    the violation of the linear model assumptions or if you feel a
    transformation of $y$ makes more sense from some theory. In this
    case we suggest you to look into `GPM=1/MPG`. Compare residual plots
    of MPG or GPM as responses and see which one might yield a more
    satisfactory patterns.

```{r data input, echo=FALSE}
library(ISLR)
data(Auto)
```

```{r variable transformations, echo=FALSE}
library(dplyr)
#use MPG as response
Auto <- Auto %>%
  dplyr::select(-name) %>%
  mutate(origin = as.factor(origin)) #remove name column and convert origin as categorical variable
fit_mpg <- lm(mpg ~ ., data = Auto)


#use GPM as response
Auto_gpm <- Auto %>%
  mutate(gpm = 1/mpg) %>%
  dplyr::select(-mpg)
fit_gpm <- lm(gpm ~ ., data = Auto_gpm)

#residual plots
par(mfrow = c(1,2))
plot(fit_mpg, 1)
plot(fit_gpm, 1)

```

**The residual plots indicate using GPM as response is more appropriate.
The residual plot of GPM (on the right) is more close to a horizontal
line, which indicates the relationship is more linear. Using GPM as
response variable meets the regression assumptions. The residual plot of
MPG suggests there may exist non-linear relationship.**

In addition, can you provide some background knowledge to support the
notion: it makes more sense to model `GPM`?

**GPM measures fuel consumed per unit of distance, whereas MPG measures
distance per unit of fuel consumed. GPM can reflect the actual
difference in fuel consumption conditioning on same miles.**

b)  You may also explore by adding interactions and higher order terms.
    The model(s) should be as *parsimonious* (simple) as possible,
    unless the gain in accuracy is significant from your point of view.

```{r check interaction, echo=FALSE}
fit_gpm1 <- lm(gpm ~ .+horsepower*origin, data = Auto_gpm)
anova(fit_gpm, fit_gpm1)
```

**Because origin might affect horsepower, we checked interaction term of
origin and horsepower. The p-value of F statistic is 0.3256, therefore,
we fail to reject** $H_0$, there is no significant difference between
the model without interaction term and model with interaction term. That
is to say, the interaction of horsepower and origin is not significant,
and we do not need to include it in our model.

c)  Use Mallow's $C_p$ or BIC to select the model.

```{r selection with Cp,  echo=FALSE}
library(leaps)
#model building
fit.exh <- regsubsets(gpm ~ ., data = Auto_gpm, nvmax = 10, method="exhaustive")
summary(fit.exh) 

#Plot the Cp values
f.e <- summary(fit.exh)
f.e$cp
plot(f.e$cp, xlab="Number of predictors",
     ylab="Cp", col="red", pch=16, cex =3,
     main = "Plot 1.1: Cp values")

```

**By using Mallow's** $C_p$, we can see a model with all 8 variables has
the smaller prediction error.

2)  Describe the final model and its accuracy. Include diagnostic plots
    with particular focus on the model residuals.

```{r,  echo=FALSE}
#Final model
fit.exh.var <- f.e$which
final_var <- colnames(fit.exh.var)[fit.exh.var[5,]]
final_var
Auto_gpm1 <- Auto_gpm %>%
  mutate(origin2 = if_else(origin == 2, "1","0")) %>%
  dplyr::select(-origin) #if origin=2, assign it = 1, else = 0

#model summary
coef(fit.exh,5)

#diagnostic plots 
plot(f.e$rss, xlab="RSS", pch = 14, col="green", cex=2, main = "Plot 1.2: RSS plot")
final_fit <- lm(gpm ~ horsepower+weight+acceleration+year+origin2, data = Auto_gpm1)
plot(final_fit,1, main = "Plot 1.3",adj=0)
plot(final_fit,2, main = "Plot 1.4",adj=0)
```

**Since the** $C_p$ values are similar in scale, we choose final model
with 5 variables. Small $C_p$ value indicates more accurate model. The
final model
is:$$gpm = 0.09662+0.0001081*horsepower+0.00001155*weight+0.00033*acceleration-0.001307year-0.00187*origin2$$

-   **From plot 1.2, we can see RSS is pretty small when selecting 5
    variables.**

-   **Plot 1.3 and 1.4 indicates the final model meets linearity and
    homoscedasticity assumption.**

    -   Summarize the effects found.

    ```{r Summarize the effects found}
    summary(final_fit)
    ```

-   $R^2$ is 0.8835 when selecting 5 variables, which suggests final
    model fits the data well.

-   **The p-values of all variables selected are significant.
    Interpretation of** $\hat{\beta_i}$: take horsepower as an example,
    controlling for other variables, gpm will increase 0.0001081 if
    there is one unit increase in horsepower.

    -   Predict the `mpg` of a car that is: built in 1983, in the US,
        red, 180 inches long, 8 cylinders, 350 displacement, 260 as
        horsepower, and weighs 4,000 pounds. Give a 95% CI.

    ```{r predict,  echo=FALSE, results='hide'}
    #car info 
    car_predict <- Auto_gpm1[1,]
    car_predict$gpm = NA
    car_predict$cylinders = 8
    car_predict$displacement = 350
    car_predict$horsepower = 260
    car_predict$weight = 4000
    car_predict$acceleration = mean(Auto_gpm1$acceleration) #no info about acceleration, use mean of acceleration in the whole data to impute missing value
    car_predict$year = 83
    car_predict$origin2 = as.factor(0)

    #predict
    car_gpm <- predict(final_fit, car_predict, interval = "confidence", se.fit = TRUE) #predicted gpm
    car_mpg <- 1/car_gpm$fit[1]
    car_mpg_ci <- c(1/car_gpm$fit[3], 1/car_gpm$fit[2]) #reciprocal of gpm ci
    car_mpg
    car_mpg_ci
    ```

    **The predicted mpg for this car is 14.8. 95% CI for mpg is (13.6,
    16.2).**

    -   Any suggestions as to how to improve the quality of the study?

    ```{r suggestions,  echo=FALSE}
    dim(Auto_gpm)
    ```

    **The sample size of the study is 392. More observations can be
    collected if possible. Also, the study could include more variables,
    such as Type of Transmission.**

# COVID-19 Case Study: Background

The outbreak of the novel Corona virus disease 2019 (COVID-19) [was
declared a public health emergency of international concern by the World
Health Organization (WHO) on January 30,
2020](https://www.who.int/dg/speeches/detail/who-director-general-s-statement-on-ihr-emergency-committee-on-novel-coronavirus-(2019-ncov)).
Upwards of [755 million cases have been confirmed worldwide, with nearly
6.8 million associated deaths](https://covid19.who.int/) by Feb of 2023.
Within the US alone, there have been [over 1.1 million deaths and
upwards of 102 million cases
reported](https://covid.cdc.gov/covid-data-tracker/#trends_dailytrendscases)
by Feb of 2023. Governments around the world have implemented and
suggested a number of policies to lessen the spread of the pandemic,
including mask-wearing requirements, travel restrictions, business and
school closures, and even stay-at-home orders. The global pandemic has
impacted the lives of individuals in countless ways, and though many
countries have begun vaccinating individuals, the long-term impact of
the virus remains unclear.

The impact of COVID-19 on a given segment of the population appears to
vary drastically based on the socioeconomic characteristics of the
segment. In particular, differing rates of infection and fatalities have
been reported among different [racial
groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html),
[age
groups](https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-age.html),
and [socioeconomic
groups](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7221360/). One of
the most important metrics for determining the impact of the pandemic is
the death rate, which is the proportion of people within the total
population that die due to the the disease.

We assemble this dataset for our research with the goal to investigate
the effectiveness of lockdown on flattening the COVID curve. We provide
a portion of the cleaned dataset for this case study.

There are two main goals for this case study.

1.  We show the dynamic evolvement of COVID cases and COVID-related
    death at state level.
2.  We try to figure out what county-level demographic and policy
    interventions are associated with mortality rate in the US. We try
    to construct models to find possible factors related to county-level
    COVID-19 mortality rates.
3.  This is a rather complex project. With our team's help we have made
    your job easier.
4.  Hide all unnecessary lengthy R-output. Keep your write up neat,
    readable.

**Remark1:** The data and the statistics reported here were collected
before February of 2021.

**Remark 2:** A group of RAs spent tremendous amount of time working
together to assemble the data. It requires data wrangling skills.

**Remark 3:** Please keep track with the most updated version of this
write-up.

# Data Summary

The data comes from several different sources:

1.  [County-level infection and fatality
    data](https://github.com/nytimes/covid-19-data) - This dataset gives
    daily cumulative numbers on infection and fatality for each county.
    -   [NYC data](https://github.com/nychealth/coronavirus-data)
2.  [County-level socioeconomic
    data](https://www.ers.usda.gov/data-products/atlas-of-rural-and-small-town-america/download-the-data/) -
    The following are the four relevant datasets from this site.
    i.  Income - Poverty level and household income.
    ii. Jobs - Employment type, rate, and change.
    iii. People - Population size, density, education level, race, age,
         household size, and migration rates.
    iv. County Classifications - Type of county (rural or urban on a
        rural-urban continuum scale).
3.  [Intervention Policy
    Data](https://github.com/JieYingWu/COVID-19_US_County-level_Summaries/blob/master/data/interventions.csv) -
    This dataset is a manually compiled list of the dates that
    interventions/lockdown policies were implemented and lifted at the
    county level.

# EDA

In this case study, we use the following three nearly cleaned data:

-   **covid_county.csv**: County-level socialeconomic information that
    combines the above-mentioned 4 datasets: Income (Poverty level and
    household income), Jobs (Employment type, rate, and change), People
    (Population size, density, education level, race, age, household
    size, and migration rates), County Classifications
-   **covid_rates.csv**: Daily cumulative numbers on infection and
    fatality for each county
-   **covid_intervention.csv**: County-level lockdown intervention.

Among all data, the unique identifier of county is `FIPS`.

The cleaning procedure is attached in `Appendix 2: Data cleaning` You
may go through it if you are interested or would like to make any
changes.

**It may need more data wrangling.**

First read in the data.

```{r read data, echo=FALSE, results='hide'}
# county-level socioeconomic information
county_data <- fread("data/covid_county.csv") 
str(county_data)
names(county_data)
skimr::skim(county_data)
sum(is.na(county_data))

table(county_data$State)

# county-level COVID case and death
covid_rate <- fread("data/covid_rates.csv")
str(covid_rate)
names(covid_rate)
sum(is.na(covid_rate))

table(covid_rate$State)
table(covid_rate$County)
table(covid_rate$date)

by(covid_rate$cum_deaths, covid_rate$State, summary)
summary(covid_rate$cum_cases)

summary(county_data$PerCapitaInc)
summary(county_data$TotalPopEst2019)
summary(county_data$ForeignBornPct)
summary(county_data$UnempRate2019)

# county-level lockdown dates 
covid_intervention <- fread("data/covid_intervention.csv")
str(covid_intervention)
names(covid_intervention)
sum(is.na(covid_intervention))
```

## Understand the data

The detailed description of variables is in
`Appendix 1: Data description`. Please get familiar with the variables.
Summarize the two data briefly.

**The dataframe "county_data" contains socioeconomic and demographic
information, in different years, of each of the 3278 counties in the
United States, which includes the counties among the 50 states and those
in Puerto Rico. While it would not be possible to summarize the entirety
of the socioeconomic dataset, a selection of variables are presented
herein. The median per capita income across all counties in the US is
\$26,720, ranging from \$5,974-\$72,832. The median county population
size as of 2019 was 26,700, ranging from 86-328,000,000. The median
percentage of residents per country who were not born in the US is 2.8,
ranging from 0-53.3%. The median county-level unemployment rate as of
2019 was 3.7%, ranging from 0.7-19.3%.**

**The dataframe "covid_rate" contains cumulative statistics for COVID
cases and deaths across 397 days (start date: 2020-01-21, end date:
2021-02-20) among the counties. The median cumulative cases by county
during this time period was 366, ranging from a minimum of 0 at the
start of the period to a maximum of 1,179,633 at the end of the period.
By scanning the data by state, we can see that individual states
experienced varying numbers of COVID cases and deaths.**

## COVID case trend

It is crucial to decide the right granularity for visualization and
analysis. We will compare daily vs weekly total new cases by state and
we will see it is hard to interpret daily report.

i)  Plot **new** COVID cases in NY, WA and FL by state and by day. Any
    irregular pattern? What is the biggest problem of using single day
    data?

**As we can see in Figure 1, it is nearly impossible to interpret more
than a year's worth of data using daily values because there are too
many data points. Although we can see slight variations in "clumps" of
higher daily rates versus lower daily rates, overall it is not feasible
to interpret data at this granularity. We can also see a few apparent
outliers, but it is challenging to draw any meaningful conclusions.
Additionally, without controlling for the population size of the state,
we can't be sure whether the number of new cases is large or small
relative to the population.**

**On a more practical note, COVID testing result release practices
varied by county which could bias any daily-level results. For example,
while most counties collected and processed COVID tests 7 days a week,
aggregated results were often only released on business days. This
resulted in apparent population-level spikes in new cases on Mondays,
when in reality this was due to the fact that test results from
Saturday, Sunday, and Monday were all released on Monday.**

```{r incidence by day, echo = FALSE}
#Subset full COVID data to NY, WA, and FL only
covid_state_subset <- covid_rate %>% filter(State == "New York" | State == "Washington" | State == "Florida")

#Create new variable so that cumulative cases never decreases
#Create new variable to indicate new cases by day
covid_state_subset <- covid_state_subset %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(cum_interim = ifelse(row_number() == 1, cum_cases, ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases))) %>%
  mutate(cum_no_dec = ifelse(row_number() == 1, cum_interim, ifelse(cum_cases < lag(cum_interim), lag(cum_interim), cum_interim)))

covid_state_subset <- covid_state_subset %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(new_cases = ifelse(row_number() == 1, cum_no_dec, cum_no_dec-lag(cum_no_dec))) %>%
  mutate(new_cases = ifelse(new_cases < 0, 0, new_cases))

#Creating plot by state
incidence_daily <- ggplot(covid_state_subset, aes(x=date, y=new_cases, color=State)) +
  geom_line() +
  labs(title = "Figure 1: ", subtitle = "New COVID-19 Cases Per Day by State", x = "Date", y = "Number of New Cases") +
  theme_bw()
incidence_daily

```

ii) Create **weekly new** cases per 100k `weekly_case_per100k`. Plot the
    spaghetti plots of `weekly_case_per100k` by state. Use
    `TotalPopEst2019` as population.

```{r incidence by week, echo = FALSE}
#Repeating above using full COVID dataset
covid_rate <- covid_rate %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(cum_interim = ifelse(row_number() == 1, cum_cases, ifelse(cum_cases < lag(cum_cases), lag(cum_cases), cum_cases))) %>%
  mutate(cum_no_dec = ifelse(row_number() == 1, cum_interim, ifelse(cum_cases < lag(cum_interim), lag(cum_interim), cum_interim)))

covid_rate <- covid_rate %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(new_cases = ifelse(row_number() == 1, cum_no_dec, cum_no_dec-lag(cum_no_dec))) %>%
  mutate(new_cases = ifelse(new_cases < 0, 0, new_cases))

#Creating new variable for weekly cases/100k population
covid_rate <- covid_rate %>%
  arrange(State, County, date) %>%
  group_by(State, County, week) %>%
  mutate(weekly_case = sum(new_cases), weekly_case_per100k = (weekly_case/TotalPopEst2019)*100000)

incidence_weekly_1 <- ggplot(subset(covid_rate, State %in% c("Alabama", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "District of Columbia", "Florida", "Georgia", "Idaho", "Illinois")), aes(x=date, y=new_cases, color=State)) +
  geom_line() +
  labs(title = "Figure 2a: ", subtitle = "New COVID-19 Cases Per Week by State", x = "Date", y = "Number of New Cases") +
  facet_wrap(~State) + 
  theme_bw() +
  theme(legend.position = "none")

incidence_weekly_2 <- ggplot(subset(covid_rate, State %in% c("Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri")), aes(x=date, y=new_cases, color=State)) +
  geom_line() +
  labs(title = "Figure 2b: ", subtitle = "New COVID-19 Cases Per Week by State", x = "Date", y = "Number of New Cases") +
  facet_wrap(~State) + 
  theme_bw() +
  theme(legend.position = "none")

incidence_weekly_3 <- ggplot(subset(covid_rate, State %in% c("Montana", "Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio", "Oklahoma", "Oregon")), aes(x=date, y=new_cases, color=State)) +
  geom_line() +
  labs(title = "Figure 2c: ", subtitle = "New COVID-19 Cases Per Week by State", x = "Date", y = "Number of New Cases") +
  facet_wrap(~State) + 
  theme_bw() +
  theme(legend.position = "none")

incidence_weekly_4 <- ggplot(subset(covid_rate, State %in% c("Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee", "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming")), aes(x=date, y=new_cases, color=State)) +
  geom_line() +
  labs(title = "Figure 2d: ", subtitle = "New COVID-19 Cases Per Week by State", x = "Date", y = "Number of New Cases") +
  facet_wrap(~State) + 
  theme_bw() +
  theme(legend.position = "none")

incidence_weekly_1
incidence_weekly_2
incidence_weekly_3
incidence_weekly_4
```

iii) Summarize the COVID case trend among states based on the plot in
     ii). What could be the possible reasons to explain the
     variabilities?

**We can see in Figures 2a-2d that each state in the US experienced
slightly different patterns regarding COVID-19 cases. One of the more
noticeable differences is in the timing of case peaks. For example, New
York appears to have one of the earliest peaks, perhaps driven by the
high population density in New York City (NYC) and presence of three
major international airports within NYC limits. In contrast,
Pennsylvania's largest peak was not until closer to 2021. It is also
evident that despite normalizing COVID-19 cases per 100k population,
there are drastic differences in cases by state. It is likely that
population density is a major driver of this difference; for example,
California has a high number of weekly cases whereas Wyoming has a very
low number of cases. Population density cannot explain all of the
variation, however, as some states with relatively high density, such as
Connecticut and Pennsylvania, have low numbers overall. It is possible
that some differences are also driven by state- and county-level
policies which restricted the flow of people and therefore the spread of
COVID-19.**

iv) (Optional) Use `covid_intervention` to see whether the effectiveness
    of lockdown in flattening the curve.

## COVID death trend

i)  For each month in 2020, plot the monthly deaths per 100k heatmap by
    state on US map. Use the same color range across months. (Hints: Set
    `limits` argument in `scale_fill_gradient()` or use `facet_wrap()`;
    use `lubridate::month()` and `lubridate::year()` to extract month
    and year from date; use
    `tidyr::complete(state, month, fill = list(new_case_per100k = NA))`
    to complete the missing months with no cases.)

```{r monthly deaths map, echo = FALSE, warning=FALSE}
#Generating variable with monthly deaths
#Creating revised cumulative deaths variables
covid_rate <- covid_rate %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(cum_interim_death = ifelse(row_number() == 1, cum_deaths, ifelse(cum_deaths < lag(cum_deaths), lag(cum_deaths), cum_deaths))) %>%
  mutate(cum_no_dec_deaths = ifelse(row_number() == 1, cum_interim_death, ifelse(cum_deaths < lag(cum_interim_death), lag(cum_interim_death), cum_interim_death)))

covid_rate <- covid_rate %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  mutate(new_deaths = ifelse(row_number() == 1, cum_no_dec_deaths, cum_no_dec_deaths-lag(cum_no_dec_deaths))) %>%
  mutate(new_deaths = ifelse(new_deaths < 0, 0, new_deaths))

#Extracting month and year from date field, dropping non-2020 data
covid_rate_2020 <- covid_rate %>% 
  mutate(year = year(date), month = month(date)) %>%
  filter(year==2020)

#Creating new variable for monthly deaths/100k population
covid_rate_2020 <- covid_rate_2020 %>%
  arrange(State, County, date) %>%
  group_by(State, County, month) %>%
  mutate(monthly_death = sum(new_deaths), monthly_death_per100k = (monthly_death/TotalPopEst2019)*100000)

covid_rate_2020 <- covid_rate_2020 %>%
  arrange(State, month) %>%
  group_by(State, month) %>%
  mutate(monthly_death_pop_state = sum(monthly_death_per100k)) %>%
  rename(state = State)

covid_rate_subset <- covid_rate_2020 %>%
  select(state, month, monthly_death_pop_state) %>%
  distinct(state, .keep_all = TRUE) %>%
  ungroup() %>%
  tidyr::complete(state, month)


#Generating static map
max_death_col <- quantile(covid_rate$monthly_death_pop_state, .96, na.rm = T)
min_death_col <- quantile(covid_rate$monthly_death_pop_state, 0, na.rm = T)

deaths_map <- plot_usmap(regions = "state", data = covid_rate_subset, values = "monthly_death_pop_state", exclude = c("Alaska", "Hawaii")) +
  scale_fill_distiller(palette = "YlOrBr", direction = 1, name = "COVID Deaths/100,000 population", limits = c(min_death_col, max_death_col), label = scales::comma) +
  facet_wrap(~month) +
  labs(title = "Figure 3: ", subtitle = "Monthly Deaths Due to COVID-19 in the Contiguous United States (2020)") +
    theme(legend.position = "bottom", legend.key.width = unit(1.75, "cm"))
deaths_map

```

ii) (Optional) Use `plotly` to animate the monthly maps in i). Does it
    reveal any systematic way to capture the dynamic changes among
    states? (Hints: Follow *Appendix 3: Plotly heatmap::* in Module 6
    regularization lecture to plot the heatmap using `plotly`. Use
    `frame` argument in `add_trace()` for animation. `plotly` only
    recognizes abbreviation of state names. Use
    `unique(us_map(regions = "states") %>% select(abbr, full))` to get
    the abbreviation and merge with the data to get state abbreviation.)

```{r plotly, echo = FALSE}
#Custom message for plotly. Can't figure out how to incorporate it, though.
pu_message <- paste0("State ", covid_rate_subset$state,
                     "<br>Deaths/100,000 population: ", round(covid_rate_subset$monthly_death_pop_state))

#Plotly map
ggplotly(deaths_map +
  theme(legend.position = "none"))
```

# COVID factor

We now try to build a good parsimonious model to find possible factors
related to death rate on county level. Let us not take time series into
account for the moment and use the total number as of *Feb 1, 2021*.

i)  Create the response variable `total_death_per100k` as the total of
    number of COVID deaths per 100k by *Feb 1, 2021*. We suggest to take
    log transformation as
    `log_total_death_per100k = log(total_death_per100k + 1)`. Merge
    `total_death_per100k` to `county_data` for the following analysis.

```{r total deaths logged and merged, echo=FALSE}
#Taking observations closest to 2/1/2021
covid_rate_feb2021 <- covid_rate %>%
  filter(date < "2021-02-01") %>%
  arrange(State, County, date) %>%
  group_by(State, County) %>%
  filter(date == max(date))

#Generating death/100k and log(death/100k), dropping unneeded variables
covid_rate_feb2021 <- covid_rate_feb2021 %>%
  mutate(total_death_per100k = (cum_no_dec_deaths/TotalPopEst2019)*100000, log_total_death_per100k = log(total_death_per100k + 1)) %>%
  select(FIPS, State, County, log_total_death_per100k)

#Merging deaths with county data
county_data <- county_data %>% rename(state_abbrev = State)
covid_county <- full_join(covid_rate_feb2021, county_data, by = c("FIPS", "County"))
  
```

ii) Select possible variables in `county_data` as covariates. We provide
    `county_data_sub`, a subset variables from `county_data`, for you to
    get started. Please add any potential variables as you wish.

    a)  Report missing values in your final subset of variables.

**Our final analysis data set includes the following missing
variables:**

-   State: None

-   log_total_death_per100k: 218

-   Deep_Pov_all: 53

-   PovertyAllAgesPct: 133

-   PerCapitaInc: 53

-   UnempRate2019: 54

-   PctEmpFIRE: 54

-   PctEmpConstruction: 54

-   PctEmpTrans: 54

-   PctEmpMining: 54

-   PctEmpTrade: 54

-   PctEmpInformation: 54

-   PctEmpAgriculture: 54

-   PctEmpManufacturing: 54

-   PctEmpServices: 54

-   PopDensity2010: 53

-   TotalOccHU: 54

-   AvgHHSize: 53

-   OwnHomePct: 53

-   Age65AndOlderPct2010: 53

-   TotalPop25Plus: 53

-   Under18Pct2010: 53

-   Ed1LessThanHSPct: 53

-   Ed2HSDiplomaOnlyPct: 53

-   Ed3SomeCollegePct: 53

-   Ed4AssocDegreePct: 53

-   Ed5CollegePlusPct: 53

-   ForeignBornPct: 132

-   Net_International_Migration_Rate_2010_2019: 132

-   NetMigrationRate1019: 132

-   NaturalChangeRate1019: 132

-   TotalPopEst2019: 53

-   WhiteNonHispanicPct2010: 53

-   NativeAmericanNonHispanicPct2010: 53

-   BlackNonHispanicPct2010: 53

-   AsianNonHispanicPct2010: 53

-   HispanicPct2010: 53

-   Type_2015_Update: 183

-   RuralUrbanContinuumCode2013: 104

-   UrbanInfluenceCode2013: 104

-   Hipov: 105

-   Perpov_1980_0711: 183

-   HiCreativeClass2000: 187

-   HiAmenity: 219

-   Retirement_Destination_2015_Update: 183

    b)  In the following anaylsis, you may ignore the missing values.

```{r county data subsetting, echo=FALSE, results='hide'}
covid_county <- covid_county %>% ungroup()
covid_county_subset <- covid_county %>%
  select(log_total_death_per100k, State, Deep_Pov_All, PovertyAllAgesPct, PerCapitaInc, UnempRate2019, PctEmpFIRE, PctEmpConstruction, PctEmpTrans, PctEmpMining, PctEmpTrade, PctEmpInformation, PctEmpAgriculture, PctEmpManufacturing, PctEmpServices, PopDensity2010, TotalOccHU, AvgHHSize, OwnHomePct, Age65AndOlderPct2010, TotalPop25Plus, Under18Pct2010, Ed1LessThanHSPct,  Ed2HSDiplomaOnlyPct, Ed3SomeCollegePct, Ed4AssocDegreePct, Ed5CollegePlusPct, ForeignBornPct, Net_International_Migration_Rate_2010_2019, NetMigrationRate1019, NaturalChangeRate1019, TotalPopEst2019, WhiteNonHispanicPct2010, NativeAmericanNonHispanicPct2010, BlackNonHispanicPct2010, AsianNonHispanicPct2010, HispanicPct2010, Type_2015_Update, RuralUrbanContinuumCode2013, UrbanInfluenceCode2013, Hipov, Perpov_1980_0711, HiCreativeClass2000, HiAmenity, Retirement_Destination_2015_Update)

summary(covid_county_subset)
```

iii) Use LASSO to choose a parsimonious model with all available
     sensible county-level information. **Force in State** in the
     process. Why we need to force in State? You may use `lambda.1se` to
     choose a smaller model.

**Using a lambda of 0.0055, our final LASSO model includes 79 variables,
which includes binary indicators for state. In our LASSO process we
needed to force in State because it is a categorical variable requiring
N-1 indicator variables. Given that, we don't want the LASSO process to
evaluate the coefficients individually, but rather as a group.**

```{r lasso, echo=FALSE, results='hide'}
set.seed(27875)

#drop NAs
covid_county_subset <- covid_county_subset[complete.cases(covid_county_subset), ]

Y <- as.matrix(covid_county_subset[, 1])
X <- model.matrix(log_total_death_per100k~., data=covid_county_subset)[, -1]
dim(X)
dim(Y)

fit.covid.cv <- cv.glmnet(X, Y, alpha=1, nfolds=10, intercept = T,
                       penalty.factor = c(rep(0, 48), rep(1, 43)))
plot(fit.covid.cv)
names(fit.covid.cv)
fit.covid.cv$lambda.min
fit.covid.cv$lambda.1se

#Based on the plot, want to use a lambda around log(-5.5), which = 0.0055. This is between lambda.min and lambda.1se so seems reasonable.

coef.force <- coef(fit.covid.cv, s=0.0055)
var.force <- coef.force@Dimnames[[1]][coef.force@i + 1][-1]
var.force

covid_final_subset <-  covid_county_subset[,c("log_total_death_per100k", "State", "PovertyAllAgesPct", "PerCapitaInc", "UnempRate2019", "PctEmpFIRE", "PctEmpConstruction", "PctEmpMining", "PctEmpTrade", "PctEmpInformation", "PctEmpAgriculture", "PctEmpManufacturing", "PctEmpServices", "PopDensity2010", "AvgHHSize", "Age65AndOlderPct2010", "TotalPop25Plus", "Under18Pct2010", "Ed1LessThanHSPct", "Ed3SomeCollegePct", "Ed5CollegePlusPct", "ForeignBornPct", "NetMigrationRate1019", "NaturalChangeRate1019", "WhiteNonHispanicPct2010", "NativeAmericanNonHispanicPct2010", "AsianNonHispanicPct2010", "HispanicPct2010", "Type_2015_Update", "RuralUrbanContinuumCode2013", "Perpov_1980_0711", "HiCreativeClass2000", "HiAmenity")]

```

```{r lasso model, echo=FALSE}
fit.covid.lm <- lm(log_total_death_per100k ~., data=covid_final_subset)
summary(fit.covid.lm) 
```

iv) Use `Cp` or BIC to fine tune the LASSO model from iii). Again
    **force in State** in the process. (You could do backward
    elimination to avoid using `Cp` or BIC)

**Further reducing the model, we now have 60 variables included in the
model, including binary indicators for state. All of the variables
(excluding some of the individual state indicator variables).**

```{r fine tune lasso, echo=FALSE, results=FALSE}
fit.final.1 <- regsubsets(log_total_death_per100k ~., nvmax = 80, force.in = c(2:48), method = "backward", covid_final_subset)
plot(summary(fit.final.1)$cp)  

fit.final.1.s <- summary(fit.final.1)
opt.size <- 12
final.var.1 <- fit.final.1.s$which
final.var <- colnames(final.var.1)[final.var.1[opt.size, ]][-1]

#Subsetting the data again with reduced variables
covid_final_subset_2 <- covid_final_subset[,c("log_total_death_per100k", "State", "PovertyAllAgesPct", "UnempRate2019", "PctEmpConstruction", "PctEmpAgriculture", "Age65AndOlderPct2010", "Under18Pct2010", "Ed3SomeCollegePct", "Ed5CollegePlusPct", "NetMigrationRate1019", "NaturalChangeRate1019", "HispanicPct2010", "RuralUrbanContinuumCode2013")]
```

```{r reduced lasso, echo=FALSE}
#Re-running the model with reduced number of parameters
fit.covid.lm.reduced_cp <- lm(log_total_death_per100k ~., data=covid_final_subset_2)
summary(fit.covid.lm.reduced_cp)
```

v)  If necessary, reduce the model from iv) to a final model with all
    variables being significant at 0.05 level. Are the linear model
    assumptions all reasonably met?

**Based on the residual plot and Q-Q plot, it is questionable whether we
can consider the assumptions for a linear model to be met. While the
residual plot appears to have a slight cone-shaped pattern, we can
probably say that homoscedasticity is reasonable met. The Q-Q plot,
however shows a fairly drastic departure from normality, indicating that
we may need to transform some of our variables to better meet this
assumption.**

```{r assumptions, echo=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(fit.covid.lm.reduced_cp, 1)
plot(fit.covid.lm.reduced_cp, 2)
```

vi) It has been shown that COVID affects elderly the most. It is also
    claimed that the COVID death rate among African Americans and
    Latinos is higher. Does your analysis support these arguments?

**Our final model did not include the percentage of Black/African
American individuals as a predictor and we are therefore unable to
determine the role this plays in COVID-19 deaths. We can see in our
model that for every percentage increase in the percentage of Hispanic
residents, counties would experience an approximately 0.9% increase in
COVID-19 deaths. Similarly, for every percentage increase in the
percentage of residents 65 and older, counties would experience an
approximately 4.4% increase in COVID-19 deaths. In both of these cases,
we can say that our analysis supports the argument that the elderly and
Hispanic individuals have been disproportionatly impacted by COVID.**

vii) Based on your final model, summarize your findings. In particular,
     summarize the state effect controlling for others. Provide
     intervention recommendations to policy makers to reduce COVID death
     rate.

**Based on our final model, we found that some states contribute more to
COVID-19 death rate than others. We also found that the net migration
and natural population change rates of the states from 2010 to 2019
contribute negatively to COVID-19 death rate, meaning that the lack of
state policies that restrict people's movement can possibly reduce
COVID-19 death rate.**

viii) What else can we do to improve our model? What other important
      information we may have missed?

**We could improve our model by including factors concerning the outdoor
environment and lifestyle of people in the counties, such as level of
air pollution and geographical landscape. This is because the county
classification information that we have is not enough to infer about
general lifestyle of the population at the county, which could
contribute to COVID case count and thus COVID death rate.**

ix) (Optional) Would your findings be very different if you had refined
    the data in some way or imputed the missing values in part ii).
    Check PCA lecture, section 10 for imputations via `softImpute`.

# Executive summary

Please summarize this project as follows (no more than one page):

**The goal of this COVID study is to investigate possible factors that relate to death rate on a county level and determine the ones that are most significant.**

**Our data is gathered from different sources including USDA and The New York Times. The first source contains socioeconomic and demographics information across US counties, which was downloaded directly from their website. The second source contains cumulative statistics for COVID cases and deaths across 397 days among the counties, which was shared and downloaded from their GitHub page.**

**We assembled the data sets by extracting only the relevant information from cumulative COVID statistics such as number of cases and deaths and considering only the counties in continental US. We also cleaned the data sets by setting NA values to 0 because they are still relevant to our analysis and NA values are more difficult to work with, and we also reformatted the dates by weeks instead of days because weekly data takes into account the variability of release dates for COVID case and death rates throughout the week. Finally, we merged the two data sets together by FIPS.**

-   Methods used

**We used**

-   Findings

-   Limitations

# Appendix 1: Data description

A detailed summary of the variables in each data set follows:

## Infection and fatality data

-   date: Date
-   county: County name
-   state: State name
-   fips: County code that uniquely identifies a county
-   cases: Number of cumulative COVID-19 infections
-   deaths: Number of cumulative COVID-19 deaths

## Socioeconomic demographics

*Income*: Poverty level and household income

-   PovertyUnder18Pct: Poverty rate for children age 0-17, 2018\
-   Deep_Pov_All: Deep poverty, 2014-18\
-   Deep_Pov_Children: Deep poverty for children, 2014-18\
-   PovertyAllAgesPct: Poverty rate, 2018\
-   MedHHInc: Median household income, 2018 (In 2018 dollars)\
-   PerCapitaInc: Per capita income in the past 12 months (In 2018
    inflation adjusted dollars), 2014-18\
-   PovertyAllAgesNum: Number of people of all ages in poverty, 2018\
-   PovertyUnder18Num: Number of people age 0-17 in poverty, 2018

*Jobs*: Employment type, rate, and change

-   UnempRate2007-2019: Unemployment rate, 2007-2019

-   NumEmployed2007-2019: Employed, 2007-2019

-   NumUnemployed2007-2019: Unemployed, 2007-2019

-   PctEmpChange1019: Percent employment change, 2010-19\

-   PctEmpChange1819: Percent employment change, 2018-19\

-   PctEmpChange0719: Percent employment change, 2007-19\

-   PctEmpChange0710: Percent employment change, 2007-10

-   NumCivEmployed: Civilian employed population 16 years and over,
    2014-18\

-   NumCivLaborforce2007-2019: Civilian labor force, 2007-2019

-   PctEmpFIRE: Percent of the civilian labor force 16 and over employed
    in finance and insurance, and real estate and rental and leasing,
    2014-18\

-   PctEmpConstruction: Percent of the civilian labor force 16 and over
    employed in construction, 2014-18\

-   PctEmpTrans: Percent of the civilian labor force 16 and over
    employed in transportation, warehousing and utilities, 2014-18

-   PctEmpMining: Percent of the civilian labor force 16 and over
    employed in mining, quarrying, oil and gas extraction, 2014-18\

-   PctEmpTrade: Percent of the civilian labor force 16 and over
    employed in wholesale and retail trade, 2014-18\

-   PctEmpInformation: Percent of the civilian labor force 16 and over
    employed in information services, 2014-18\

-   PctEmpAgriculture: Percent of the civilian labor force 16 and over
    employed in agriculture, forestry, fishing, and hunting, 2014-18\

-   PctEmpManufacturing: Percent of the civilian labor force 16 and over
    employed in manufacturing, 2014-18\

-   PctEmpServices: Percent of the civilian labor force 16 and over
    employed in services, 2014-18\

-   PctEmpGovt: Percent of the civilian labor force 16 and over employed
    in public administration, 2014-18

*People*: Population size, density, education level, race, age,
household size, and migration rates

-   PopDensity2010: Population density, 2010\

-   LandAreaSQMiles2010: Land area in square miles, 2010

-   TotalHH: Total number of households, 2014-18\

-   TotalOccHU: Total number of occupied housing units, 2014-18\

-   AvgHHSize: Average household size, 2014-18\

-   OwnHomeNum: Number of owner occupied housing units, 2014-18\

-   OwnHomePct: Percent of owner occupied housing units, 2014-18

-   NonEnglishHHPct: Percent of non-English speaking households of total
    households, 2014-18\

-   HH65PlusAlonePct: Percent of persons 65 or older living alone,
    2014-18\

-   FemaleHHPct: Percent of female headed family households of total
    households, 2014-18\

-   FemaleHHNum: Number of female headed family households, 2014-18\

-   NonEnglishHHNum: Number of non-English speaking households, 2014-18\

-   HH65PlusAloneNum: Number of persons 65 years or older living alone,
    2014-18

-   Age65AndOlderPct2010: Percent of population 65 or older, 2010

-   Age65AndOlderNum2010: Population 65 years or older, 2010\

-   TotalPop25Plus: Total population 25 and older, 2014-18 - 5-year
    average\

-   Under18Pct2010: Percent of population under age 18, 2010\

-   Under18Num2010: Population under age 18, 2010

-   Ed1LessThanHSPct: Percent of persons with no high school diploma or
    GED, adults 25 and over, 2014-18\

-   Ed2HSDiplomaOnlyPct: Percent of persons with a high school diploma
    or GED only, adults 25 and over, 2014-18\

-   Ed3SomeCollegePct: Percent of persons with some college experience,
    adults 25 and over, 2014-18\

-   Ed4AssocDegreePct: Percent of persons with an associate's degree,
    adults 25 and over, 2014-18\

-   Ed5CollegePlusPct: Percent of persons with a 4-year college degree
    or more, adults 25 and over, 2014-18\

-   Ed1LessThanHSNum: No high school, adults 25 and over, 2014-18

-   Ed2HSDiplomaOnlyNum: High school only, adults 25 and over, 2014-18\

-   Ed3SomeCollegeNum: Some college experience, adults 25 and over,
    2014-18\

-   Ed4AssocDegreeNum: Number of persons with an associate's degree,
    adults 25 and over, 2014-18\

-   Ed5CollegePlusNum: College degree 4-years or more, adults 25 and
    over, 2014-18

-   ForeignBornPct: Percent of total population foreign born, 2014-18\

-   ForeignBornEuropePct: Percent of persons born in Europe, 2014-18\

-   ForeignBornMexPct: Percent of persons born in Mexico, 2014-18

-   ForeignBornCentralSouthAmPct: Percent of persons born in Central or
    South America, 2014-18\

-   ForeignBornAsiaPct: Percent of persons born in Asia, 2014-18

-   ForeignBornCaribPct: Percent of persons born in the Caribbean,
    2014-18\

-   ForeignBornAfricaPct: Percent of persons born in Africa, 2014-18\

-   ForeignBornNum: Number of people foreign born, 2014-18\

-   ForeignBornCentralSouthAmNum: Number of persons born in Central or
    South America, 2014-18\

-   ForeignBornEuropeNum: Number of persons born in Europe, 2014-18\

-   ForeignBornMexNum: Number of persons born in Mexico, 2014-18

-   ForeignBornAfricaNum: Number of persons born in Africa, 2014-18\

-   ForeignBornAsiaNum: Number of persons born in Asia, 2014-18

-   ForeignBornCaribNum: Number of persons born in the Caribbean,
    2014-18

-   Net_International_Migration_Rate_2010_2019: Net international
    migration rate, 2010-19\

-   Net_International_Migration_2010_2019: Net international migration,
    2010-19\

-   Net_International_Migration_2000_2010: Net international migration,
    2000-10\

-   Immigration_Rate_2000_2010: Net international migration rate,
    2000-10\

-   NetMigrationRate0010: Net migration rate, 2000-10\

-   NetMigrationRate1019: Net migration rate, 2010-19\

-   NetMigrationNum0010: Net migration, 2000-10\

-   NetMigration1019: Net Migration, 2010-19

-   NaturalChangeRate1019: Natural population change rate, 2010-19\

-   NaturalChangeRate0010: Natural population change rate, 2000-10\

-   NaturalChangeNum0010: Natural change, 2000-10\

-   NaturalChange1019: Natural population change, 2010-19

-   TotalPop2010: Population size 4/1/2010 Census

-   TotalPopEst2010: Population size 7/1/2010

-   TotalPopEst2011: Population size 7/1/2011

-   TotalPopEst2012: Population size 7/1/2012

-   TotalPopEst2013: Population size 7/1/2013

-   TotalPopEst2014: Population size 7/1/2014

-   TotalPopEst2015: Population size 7/1/2015

-   TotalPopEst2016: Population size 7/1/2016

-   TotalPopEst2017: Population size 7/1/2017

-   TotalPopEst2018: Population size 7/1/2018

-   TotalPopEst2019: Population size 7/1/2019

-   TotalPopACS: Total population, 2014-18 - 5-year average\

-   TotalPopEstBase2010: County Population estimate base 4/1/2010

-   NonHispanicAsianPopChangeRate0010: Population change rate
    Non-Hispanic Asian, 2000-10\

-   PopChangeRate1819: Population change rate, 2018-19\

-   PopChangeRate1019: Population change rate, 2010-19\

-   PopChangeRate0010: Population change rate, 2000-10\

-   NonHispanicNativeAmericanPopChangeRate0010: Population change rate
    Non-Hispanic Native American, 2000-10\

-   HispanicPopChangeRate0010: Population change rate Hispanic, 2000-10\

-   MultipleRacePopChangeRate0010: Population change rate multiple race,
    2000-10\

-   NonHispanicWhitePopChangeRate0010: Population change rate
    Non-Hispanic White, 2000-10\

-   NonHispanicBlackPopChangeRate0010: Population change rate
    Non-Hispanic African American, 2000-10

-   MultipleRacePct2010: Percent multiple race, 2010\

-   WhiteNonHispanicPct2010: Percent Non-Hispanic White, 2010\

-   NativeAmericanNonHispanicPct2010: Percent Non-Hispanic Native
    American, 2010\

-   BlackNonHispanicPct2010: Percent Non-Hispanic African American,
    2010\

-   AsianNonHispanicPct2010: Percent Non-Hispanic Asian, 2010\

-   HispanicPct2010: Percent Hispanic, 2010\

-   MultipleRaceNum2010: Population size multiple race, 2010\

-   WhiteNonHispanicNum2010: Population size Non-Hispanic White, 2010\

-   BlackNonHispanicNum2010: Population size Non-Hispanic African
    American, 2010\

-   NativeAmericanNonHispanicNum2010: Population size Non-Hispanic
    Native American, 2010\

-   AsianNonHispanicNum2010: Population size Non-Hispanic Asian, 2010\

-   HispanicNum2010: Population size Hispanic, 2010

##County classifications

Type of county (rural or urban on a rural-urban continuum scale)

-   Type_2015_Recreation_NO: Recreation counties, 2015 edition\

-   Type_2015_Farming_NO: Farming-dependent counties, 2015 edition\

-   Type_2015_Mining_NO: Mining-dependent counties, 2015 edition

-   Type_2015_Government_NO: Federal/State government-dependent
    counties, 2015 edition\

-   Type_2015_Update: County typology economic types, 2015 edition\

-   Type_2015_Manufacturing_NO: Manufacturing-dependent counties, 2015
    edition\

-   Type_2015_Nonspecialized_NO: Nonspecialized counties, 2015 edition\

-   RecreationDependent2000: Nonmetro recreation-dependent, 1997-00\

-   ManufacturingDependent2000: Manufacturing-dependent, 1998-00

-   FarmDependent2003: Farm-dependent, 1998-00\

-   EconomicDependence2000: Economic dependence, 1998-00

-   RuralUrbanContinuumCode2003: Rural-urban continuum code, 2003

-   UrbanInfluenceCode2003: Urban influence code, 2003\

-   RuralUrbanContinuumCode2013: Rural-urban continuum code, 2013

-   UrbanInfluenceCode2013: Urban influence code, 2013\

-   Noncore2013: Nonmetro noncore, outside Micropolitan and
    Metropolitan, 2013\

-   Micropolitan2013: Micropolitan, 2013

-   Nonmetro2013: Nonmetro, 2013

-   Metro2013: Metro, 2013\

-   Metro_Adjacent2013: Nonmetro, adjacent to metro area, 2013\

-   Noncore2003: Nonmetro noncore, outside Micropolitan and
    Metropolitan, 2003\

-   Micropolitan2003: Micropolitan, 2003\

-   Metro2003: Metro, 2003\

-   Nonmetro2003: Nonmetro, 2003\

-   NonmetroNotAdj2003: Nonmetro, nonadjacent to metro area, 2003

-   NonmetroAdj2003: Nonmetro, adjacent to metro area, 2003

-   Oil_Gas_Change: Change in the value of onshore oil and natural gas
    production, 2000-11\

-   Gas_Change: Change in the value of onshore natural gas production,
    2000-11\

-   Oil_Change: Change in the value of onshore oil production, 2000-11

-   Hipov: High poverty counties, 2014-18\

-   Perpov_1980_0711: Persistent poverty counties, 2015 edition\

-   PersistentChildPoverty_1980_2011: Persistent child poverty counties,
    2015 edition\

-   PersistentChildPoverty2004: Persistent child poverty counties, 2004\

-   PersistentPoverty2000: Persistent poverty counties, 2004

-   Low_Education_2015_update: Low education counties, 2015 edition

-   LowEducation2000: Low education, 2000

-   HiCreativeClass2000: Creative class, 2000\

-   HiAmenity: High natural amenities\

-   RetirementDestination2000: Retirement destination, 1990-00\

-   Low_Employment_2015_update: Low employment counties, 2015 edition

-   Population_loss_2015_update: Population loss counties, 2015 edition

-   Retirement_Destination_2015_Update: Retirement destination counties,
    2015 edition

# Appendix 2: Data cleaning

The raw data sets are dirty and need transforming before we can do our
EDA. It takes time and efforts to clean and merge different data sources
so we provide the final output of the cleaned and merged data. The
cleaning procedure is as follows. Please read through to understand what
is in the cleaned data. We set `eval = data_cleaned` in the following
cleaning chunks so that these cleaning chunks will only run if any of
`data/covid_county.csv`, `data/covid_rates.csv` or
`data/covid_intervention.csv` does not exist.

```{r}
# Indicator to check whether the data files exist
data_cleaned <- !(file.exists("data/covid_county.csv") & 
                    file.exists("data/covid_rates.csv") & 
                    file.exists("data/covid_intervention.csv"))
```

We first read in the table using `data.table::fread()`, as we did last
time.

```{r Read in covid data, eval = data_cleaned}
# COVID case/mortality rate data
covid_rates <- fread("data/us_counties.csv", na.strings = c("NA", "", "."))
nyc <- fread("data/nycdata.csv", na.strings = c("NA", "", "."))

# Socioeconomic data
income <- fread("data/income.csv", na.strings = c("NA", "", "."))
jobs <- fread("data/jobs.csv", na.strings = c("NA", "", "."))
people <- fread("data/people.csv", na.strings = c("NA", "", "."))
county_class <- fread("data/county_classifications.csv", na.strings = c("NA", "", "."))

# Internvention policy data
int_dates <- fread("data/intervention_dates.csv", na.strings = c("NA", "", "."))
```

## Clean NYC data

The original NYC data contains more information than we need. We extract
only the number of cases and deaths and format it the same as the
`covid_rates` data.

```{r Clean NYC data, eval = data_cleaned}
# NYC county fips matching table 
nyc_fips <- data.table(FIPS = c('36005', '36047', '36061', '36081', '36085'),
                       County = c("BX", "BK", "MN", "QN", "SI"))

# nyc case
nyc_case <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_CASE_COUNT, 
                   BK = BK_CASE_COUNT, 
                   MN = MN_CASE_COUNT, 
                   QN = QN_CASE_COUNT, 
                   SI = SI_CASE_COUNT)]

nyc_case %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "cases") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_cases = cumsum(cases))

# nyc death
nyc_death <- nyc[,.(date = as.Date(date_of_interest, "%m/%d/%Y"),
                   BX = BX_DEATH_COUNT, 
                   BK = BK_DEATH_COUNT, 
                   MN = MN_DEATH_COUNT, 
                   QN = QN_DEATH_COUNT, 
                   SI = SI_DEATH_COUNT)]

nyc_death %<>% 
  pivot_longer(cols = BX:SI, 
               names_to = "County",
               values_to = "deaths") %>%
  arrange(date) %>%
  group_by(County) %>%
  mutate(cum_deaths = cumsum(deaths))

nyc_rates <- merge(nyc_case, 
                   nyc_death,
                   by = c("date", "County"),
                   all.x= T)

nyc_rates <- merge(nyc_rates,
                   nyc_fips,
                   by = "County")

nyc_rates$State <- "New York"
nyc_rates %<>% 
  select(date, FIPS, County, State, cum_cases, cum_deaths) %>%
  arrange(FIPS, date)
```

## Continental US cases

We only consider cases and death in continental US. Alaska, Hawaii, and
Puerto Rico have 02, 15, and 72 as their respective first 2 digits of
their FIPS. We use the `%/%` operator for integer division to get the
first 2 digits of FIPS. We also remove Virgin Islands and Northern
Mariana Islands. All data of counties in NYC are aggregated as
`County == "New York City"` in `covid_rates` with no FIPS, so we combine
the NYC data into `covid_rate`.

```{r, eval = data_cleaned}
covid_rates <- covid_rates %>% 
  arrange(fips, date) %>%
  filter(!(fips %/% 1000 %in% c(2, 15, 72))) %>%
  filter(county != "New York City") %>%
  filter(!(state %in% c("Virgin Islands", "Northern Mariana Islands"))) %>%
  rename(FIPS = "fips",
         County = "county",
         State = "state",
         cum_cases = "cases", 
         cum_deaths = "deaths")


covid_rates$date <- as.Date(covid_rates$date)

covid_rates <- rbind(covid_rates, 
                     nyc_rates)
```

## COVID date to week

We set the week of Jan 21, 2020 (the first case of COVID case in US) as
the first week (2020-01-19 to 2020-01-25).

```{r, eval = data_cleaned}
covid_rates[, week := (interval("2020-01-19", date) %/% weeks(1)) + 1]
```

## COVID infection/mortality rates

Merge the `TotalPopEst2019` variable from the demographic data with
`covid_rates` by FIPS.

```{r, eval = data_cleaned}
covid_rates <- merge(covid_rates[!is.na(FIPS)], 
                     people[,.(FIPS = as.character(FIPS),
                                   TotalPopEst2019)],
                     by = "FIPS",  
                     all.x = TRUE)
```

## NA in COVID data

NA values in the `covid_rates` data set correspond to a county not
having confirmed cases/deaths. We replace the NA values in these columns
with zeros. FIPS for Kansas city, Missouri, Rhode Island and some others
are missing. We drop them for the moment and output the data up to week
57 as `covid_rates.csv`.

```{r, eval = data_cleaned}
covid_rates$cum_cases[is.na(covid_rates$cum_cases)] <- 0
covid_rates$cum_deaths[is.na(covid_rates$cum_deaths)] <- 0
```

```{r, eval = data_cleaned}
fwrite(covid_rates %>% 
         filter(week < 58) %>% 
         arrange(FIPS, date), 
       "data/covid_rates.csv")
```

## Formatting date in `int_dates`

We convert the columns representing dates in `int_dates` to R Date types
using `as.Date()`. We will need to specify that the `origin` parameter
is `"0001-01-01"`. We output the data as `covid_intervention.csv`.

```{r, eval = data_cleaned}
int_dates <- int_dates[-1,]
date_cols <- names(int_dates)[-(1:3)]
int_dates[, (date_cols) := lapply(.SD, as.Date, origin = "0001-01-01"), 
          .SDcols = date_cols]

fwrite(int_dates, "data/covid_intervention.csv")
```

## Merge demographic data

Merge the demographic data sets by FIPS and output as
`covid_county.csv`.

```{r, eval = data_cleaned}
countydata <-
  merge(x = income,
        y = merge(
          x = people,
          y = jobs,
          by = c("FIPS", "State", "County")),
        by = c("FIPS", "State", "County"),
        all = TRUE)


countydata <- 
  merge(
    x = countydata,
    y = county_class %>% rename(FIPS = FIPStxt), 
    by = c("FIPS", "State", "County"),
    all = TRUE
  )

# Check dimensions
# They are now 3279 x 208
dim(countydata)
fwrite(countydata, "data/covid_county.csv")
```
